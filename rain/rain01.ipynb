{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84a291e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GridSearchCV\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m \u001B[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/__init__.py:109\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parse \u001B[38;5;28;01mas\u001B[39;00m parse_version\n\u001B[1;32m    107\u001B[0m \u001B[38;5;66;03m# cbook must import matplotlib only within function\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;66;03m# definitions, so it is safe to import from it here.\u001B[39;00m\n\u001B[0;32m--> 109\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, _version, cbook, docstring, rcsetup\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mplDeprecation  \u001B[38;5;66;03m# deprecated\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/rcsetup.py:27\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, cbook\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ls_mapper\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Colormap, is_color_like\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfontconfig_pattern\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parse_fontconfig_pattern\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_enums\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JoinStyle, CapStyle\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.9/site-packages/matplotlib/colors.py:51\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumbers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Number\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mPngImagePlugin\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PngInfo\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmpl\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/uciml/glass\n",
    "import pyforest\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Purpose\n",
    "is to try to predict the rainy days basing on the atmospheric pressure changes\n",
    "I got the data for the year 2021 from this website: http://meteo.ftj.agh.edu.pl/archivalData"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading Input Data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA = pd.read_csv(\"input/data2.csv\", sep=';')\n",
    "DATA.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Exploring the Data, planning the preprocessing\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DATA.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA['time'] = pd.to_datetime(DATA['time'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "DATA.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Columns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from libs.simpleplotter import simple_features_overview\n",
    "simple_features_overview(DATA)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlations between data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "data_correlations = DATA.corr()\n",
    "fig, ax = plt.subplots(figsize=(22, 20))\n",
    "sns.heatmap(data_correlations, annot=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from libs.simpleplotter import simple_correlations\n",
    "# simple_correlations(DATA, \"rainIntensity\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing, Categorical & Not Useful Data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10465696",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, columns: Ticket, Name will be dropped as they have only unique strings, so they can't become Categorical columns\n",
    "Sex, Embarked - will be converted to Categorical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585efe9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65cbc8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27905c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's run the preprocessing on both train and test data\n",
    "\n",
    "PROCESSED = DATA.copy(deep=True)\n",
    "\n",
    "# Filling-up empty records\n",
    "PROCESSED['minPm10'].fillna(PROCESSED[\"minPm10\"].mean(), inplace=True)\n",
    "PROCESSED['maxPm10'].fillna(PROCESSED[\"maxPm10\"].mean(), inplace=True)\n",
    "PROCESSED['averagePm10'].fillna(PROCESSED[\"averagePm10\"].mean(), inplace=True)\n",
    "PROCESSED['maxWindDirection'].fillna(PROCESSED[\"maxWindDirection\"].mean(), inplace=True)\n",
    "PROCESSED['averageWindDirection'].fillna(PROCESSED[\"averageWindDirection\"].mean(), inplace=True)\n",
    "PROCESSED['minWindDirection'].fillna(PROCESSED[\"minWindDirection\"].mean(), inplace=True)\n",
    "PROCESSED['maxWindSpeed'].fillna(PROCESSED[\"maxWindSpeed\"].mean(), inplace=True)\n",
    "PROCESSED['minWindSpeed'].fillna(PROCESSED[\"minWindSpeed\"].mean(), inplace=True)\n",
    "PROCESSED['averageWindSpeed'].fillna(PROCESSED[\"averageWindSpeed\"].mean(), inplace=True)\n",
    "\n",
    "# copy of the time column, for predictions\n",
    "time = PROCESSED['time'].copy()\n",
    "PROCESSED.drop('time', inplace=True, axis='columns')\n",
    "\n",
    "# I want to use classificators therefore I need to create is_raining column with binary data\n",
    "\n",
    "PROCESSED['is_rain'] = np.where(PROCESSED['rainAccumulation']>0, 1, 0)\n",
    "\n",
    "PROCESSED.drop(['rainAccumulation', 'rainIntensity'], inplace=True, axis='columns')\n",
    "\n",
    "\n",
    "PROCESSED.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "PROCESSED.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "91cc684f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Post-Processing Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d63e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "data_correlations = PROCESSED.corr()\n",
    "plt.subplots(figsize=(22, 20))\n",
    "sns.heatmap(data_correlations, cmap = 'Blues', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing for Model Training\n",
    "\n",
    "## Separating target from features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b704d4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = PROCESSED['is_rain']\n",
    "X = PROCESSED.drop(['is_rain'], axis=1)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting train data into train & validation data\n",
    "as we can see the number of records in train data is lowered"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c3498",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.5, test_size=0.5, random_state=0)\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139f3ba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models Training & Hyper-params Tuning for Different Classification Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d9fa4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# svc=SVC(probability=True, kernel='linear')\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# helper function to retrieve model name from model object\n",
    "from libs.simple_processing import get_model_name\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'hyperparams': {\n",
    "            'n_neighbors': range(16, 20, 1),\n",
    "            'n_jobs': range(4, 5, 1)\n",
    "        }\n",
    "    },\n",
    "    # {\n",
    "    #     'model': DecisionTreeClassifier(),\n",
    "    #     'hyperparams': {\n",
    "    #         'max_depth': [1, 2, 3, 4, 5]\n",
    "    #     }\n",
    "    # },\n",
    "    # {\n",
    "    #     'model': RandomForestClassifier(n_jobs=4, max_features=\"auto\", bootstrap=False),\n",
    "    #     'hyperparams': {\n",
    "    #         'criterion' :['gini', 'entropy'],\n",
    "    #         'n_estimators': range(100, 300, 50),\n",
    "    #         'n_jobs': range(4, 5, 1),\n",
    "    #         'max_depth': range(6, 9, 1)\n",
    "    #     }\n",
    "    # },\n",
    "    {\n",
    "        'model': GradientBoostingClassifier(random_state=0, max_features=\"auto\"),\n",
    "        'hyperparams': {\n",
    "            'n_estimators': range(15, 25, 1),\n",
    "            'learning_rate': np.arange(0.01, 0.1, 0.01),\n",
    "            'max_depth': range(1, 5, 1)\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_model = {}\n",
    "\n",
    "# also tried to use cv=4 and 6-15 but it made results worse\n",
    "for p in params:\n",
    "    print('> Model:', get_model_name(p['model']))\n",
    "    grid_model = GridSearchCV(p['model'], p['hyperparams'], cv=5, n_jobs=-1, scoring='accuracy', verbose=1)\n",
    "    grid_model.fit(X_train, y_train)\n",
    "    print('Best params:', grid_model.best_params_)\n",
    "    print('Best score:', grid_model.best_score_)\n",
    "\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyper-params Tuning Summary\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Winner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=4)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_valid)\n",
    "pred_results = pd.DataFrame({'y_valid' : y_valid, 'y_pred': y_pred})\n",
    "pred_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(learning_rate=0.09, max_depth=4, n_estimators=24)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_valid)\n",
    "pred_results = pd.DataFrame({'y_valid' : y_valid, 'y_pred': y_pred})\n",
    "\n",
    "pred_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Score\n",
    "Accuracy score / Dokładność [(TP+TN) / (TP+TN+FP+FN)]: how many of the values were predicted correctly?\n",
    "Accuracy count: number of correct predictions\n",
    "Precision score / Precyzja [TP / (TP+FP)]: how many passengers that the model thought survived actually did survive?\n",
    "Recall score / Pełność [TP / (TP+FN]: how many of the actual survivors the model correctly predicted?\n",
    "(also known as true positive rate / wskaźnik skuteczności - ryzyko niepoprawnego oznaczenia ofiary wypadku)\n",
    "F1 score [2(prec x rec) / (prec + rec)]: combination of the above precision and recall scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from libs.simple_processing import print_scores\n",
    "\n",
    "print_scores(y_valid, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Receiver Operating Characteristic (ROC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from libs.simpleplotter import simple_roc\n",
    "\n",
    "simple_roc(y_valid, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix / Macierz pomyłek"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's take a look at the results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from libs.simpleplotter import simple_confusion_matrix\n",
    "# conf_matrix = confusion_matrix(y_true=y_valid, y_pred=y_pred)\n",
    "# conf_matrix\n",
    "conf_matrix = simple_confusion_matrix(y_valid, y_pred, model.classes_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# crosstab = pd.crosstab(pred_results.y_pred, pred_results.y_valid)\n",
    "print('TP:', conf_matrix[1][1])\n",
    "print('TN:', conf_matrix[0][0])\n",
    "print('FP:', conf_matrix[0][1])\n",
    "print('FN:', conf_matrix[1][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Which features became decision makers for the model?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from libs.simpleplotter import feature_importance\n",
    "\n",
    "feature_importance(model, X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "c21dc002",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generating Model Predictions For Test Data & Saving the results for Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# y_test = model.predict(X)\n",
    "# output = pd.DataFrame({'Id': IDs, 'Species': y_test})\n",
    "# output.to_csv('./submission.csv', index=False)\n",
    "# SUBMISSION = pd.read_csv(\"./submission.csv\")\n",
    "# SUBMISSION"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3.9v2",
   "language": "python",
   "display_name": "Python 3.9 v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}